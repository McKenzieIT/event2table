#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½ç¼“å­˜é¢„çƒ­ç³»ç»Ÿ
================

åŸºäºå†å²è®¿é—®æ•°æ®é¢„æµ‹çƒ­ç‚¹é”®å¹¶æå‰é¢„çƒ­

ç‰ˆæœ¬: 1.0.0
æ—¥æœŸ: 2026-02-24

åŠŸèƒ½:
- è®¿é—®æ—¥å¿—è®°å½•
- çƒ­ç‚¹é”®é¢„æµ‹ (åŸºäºé¢‘ç‡å’Œè¶‹åŠ¿)
- å¯åŠ¨æ—¶é¢„çƒ­ (Top 100)
- å®šæ—¶é¢„çƒ­ (æ¯5åˆ†é’Ÿ)
- ç®€å•é¢„æµ‹ç®—æ³• (å¯æ‰©å±•ä¸ºARIMA)
"""

from collections import defaultdict, deque
from typing import Dict, List, Optional, Callable, TYPE_CHECKING
import threading
import time
import logging

if TYPE_CHECKING:
    from .cache_hierarchical import HierarchicalCache

try:
    from .cache_hierarchical import hierarchical_cache
    from .cache_system import CacheKeyBuilder, get_cache
except ImportError:
    hierarchical_cache = None  # type: ignore
    CacheKeyBuilder = None  # type: ignore
    get_cache = None  # type: ignore

logger = logging.getLogger(__name__)


class CircularBuffer:
    """å¾ªç¯ç¼“å†²åŒº"""

    def __init__(self, size: int):
        """
        åˆå§‹åŒ–å¾ªç¯ç¼“å†²åŒº

        Args:
            size: ç¼“å†²åŒºå¤§å°
        """
        self.buffer: deque = deque(maxlen=size)
        self._lock = threading.Lock()

    def append(self, item):
        """æ·»åŠ é¡¹"""
        with self._lock:
            self.buffer.append(item)

    def get_items(self, count: Optional[int] = None) -> List:
        """
        è·å–é¡¹

        Args:
            count: æ•°é‡ (Noneè¡¨ç¤ºå…¨éƒ¨)

        Returns:
            é¡¹åˆ—è¡¨
        """
        with self._lock:
            if count is None:
                return list(self.buffer)
            else:
                return list(self.buffer)[-count:]

    def __len__(self):
        """è·å–é•¿åº¦"""
        return len(self.buffer)


class FrequencyPredictor:
    """åŸºäºé¢‘ç‡çš„ç®€å•é¢„æµ‹å™¨"""

    def predict(
        self,
        key_frequency: Dict[str, int],
        top_n: int = 100
    ) -> List[str]:
        """
        é¢„æµ‹çƒ­ç‚¹é”®

        Args:
            key_frequency: é”®é¢‘ç‡å­—å…¸
            top_n: è¿”å›å‰Nä¸ªçƒ­ç‚¹é”®

        Returns:
            çƒ­ç‚¹é”®åˆ—è¡¨ (æŒ‰é¢‘ç‡é™åº)
        """
        # æŒ‰é¢‘ç‡æ’åº
        sorted_keys = sorted(
            key_frequency.items(),
            key=lambda x: x[1],
            reverse=True
        )

        return [key for key, _ in sorted_keys[:top_n]]

    def predict_with_decay(
        self,
        access_log: List[Dict],
        top_n: int = 100,
        decay_factor: float = 0.95
    ) -> List[str]:
        """
        é¢„æµ‹çƒ­ç‚¹é”® (å¸¦æ—¶é—´è¡°å‡)

        Args:
            access_log: è®¿é—®æ—¥å¿—
            top_n: è¿”å›å‰Nä¸ªçƒ­ç‚¹é”®
            decay_factor: è¡°å‡å› å­ (è¶Šè¿‘çš„è®¿é—®æƒé‡è¶Šé«˜)

        Returns:
            çƒ­ç‚¹é”®åˆ—è¡¨
        """
        if not access_log:
            return []

        # è®¡ç®—åŠ æƒé¢‘ç‡
        key_scores: Dict[str, float] = defaultdict(float)
        current_time = time.time()

        for access in access_log:
            key = access['key']
            timestamp = access['timestamp']

            # æ—¶é—´è¡°å‡
            age_seconds = current_time - timestamp
            age_hours = age_seconds / 3600

            # è®¡ç®—æƒé‡
            weight = decay_factor ** age_hours
            key_scores[key] += weight

        # æŒ‰åŠ æƒåˆ†æ•°æ’åº
        sorted_keys = sorted(
            key_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )

        return [key for key, _ in sorted_keys[:top_n]]


class IntelligentCacheWarmer:
    """
    æ™ºèƒ½ç¼“å­˜é¢„çƒ­å™¨

    é¢„çƒ­ç­–ç•¥:
    1. å¯åŠ¨æ—¶é¢„çƒ­: ä»å†å²æ—¥å¿—æå–Top 100çƒ­ç‚¹é”®
    2. å®šæ—¶é¢„çƒ­: æ¯5åˆ†é’Ÿé¢„æµ‹æœªæ¥çƒ­ç‚¹å¹¶é¢„çƒ­
    3. å®æ—¶é¢„çƒ­: æ£€æµ‹åˆ°çªå‘æµé‡æ—¶è‡ªåŠ¨é¢„çƒ­

    é¢„æµ‹ç®—æ³•:
    - åŸºç¡€: é¢‘ç‡ç»Ÿè®¡
    - è¿›é˜¶: æ—¶é—´è¡°å‡
    - æœªæ¥: ARIMAæ—¶é—´åºåˆ—é¢„æµ‹
    """

    def __init__(
        self,
        access_log_size: int = 10000,
        warm_up_interval: int = 300  # 5åˆ†é’Ÿ
    ):
        """
        åˆå§‹åŒ–é¢„çƒ­å™¨

        Args:
            access_log_size: è®¿é—®æ—¥å¿—å¤§å°
            warm_up_interval: é¢„çƒ­é—´éš” (ç§’)
        """
        self.access_log: CircularBuffer = CircularBuffer(access_log_size)
        self.predictor: FrequencyPredictor = FrequencyPredictor()
        self.warm_up_interval: int = warm_up_interval

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats: Dict[str, float] = {
            'warm_up_count': 0.0,
            'keys_warmed': 0.0,
            'last_warm_up_time': 0.0,
            'prediction_accuracy': 0.0,  # TODO: è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡
        }

        self._lock = threading.Lock()

        logger.info("âœ… æ™ºèƒ½ç¼“å­˜é¢„çƒ­å™¨åˆå§‹åŒ–å®Œæˆ")

    def record_access(self, key: str):
        """
        è®°å½•ç¼“å­˜è®¿é—®

        Args:
            key: ç¼“å­˜é”®
        """
        self.access_log.append({
            'key': key,
            'timestamp': time.time()
        })

    def predict_hot_keys(
        self,
        minutes: int = 5,
        top_n: int = 100,
        use_decay: bool = True
    ) -> List[str]:
        """
        é¢„æµ‹æœªæ¥Nåˆ†é’Ÿçš„çƒ­ç‚¹é”®

        Args:
            minutes: é¢„æµ‹æœªæ¥åˆ†é’Ÿæ•°
            top_n: è¿”å›å‰Nä¸ªçƒ­ç‚¹é”®
            use_decay: æ˜¯å¦ä½¿ç”¨æ—¶é—´è¡°å‡

        Returns:
            çƒ­ç‚¹é”®åˆ—è¡¨
        """
        # è·å–æœ€è¿‘1å°æ—¶çš„è®¿é—®è®°å½•
        cutoff_time = time.time() - 3600
        recent_access = [
            access for access in self.access_log.get_items()
            if access['timestamp'] >= cutoff_time
        ]

        if not recent_access:
            logger.debug("æ²¡æœ‰å†å²è®¿é—®æ•°æ®ï¼Œæ— æ³•é¢„æµ‹çƒ­ç‚¹é”®")
            return []

        # é¢„æµ‹çƒ­ç‚¹é”®
        if use_decay:
            hot_keys = self.predictor.predict_with_decay(
                recent_access,
                top_n=top_n
            )
        else:
            # ç»Ÿè®¡é¢‘ç‡
            key_frequency: Dict[str, int] = defaultdict(int)
            for access in recent_access:
                key_frequency[access['key']] += 1

            hot_keys = self.predictor.predict(
                dict(key_frequency),
                top_n=top_n
            )

        logger.info(
            f"ğŸ”® é¢„æµ‹æœªæ¥{minutes}åˆ†é’Ÿçš„çƒ­ç‚¹é”®: "
            f"{len(hot_keys)}ä¸ª"
        )

        return hot_keys

    async def warm_up_cache(
        self,
        keys: List[str],
        fetch_callback: Optional[Callable] = None
    ) -> Dict:
        """
        é¢„çƒ­ç¼“å­˜

        Args:
            keys: è¦é¢„çƒ­çš„é”®åˆ—è¡¨
            fetch_callback: ä»æ•°æ®åº“è·å–æ•°æ®çš„å›è°ƒå‡½æ•°

        Returns:
            é¢„çƒ­ç»Ÿè®¡
        """
        if not keys:
            return {'warmed': 0, 'failed': 0, 'skipped': 0}

        warmed = 0
        failed = 0
        skipped = 0

        for key in keys:
            try:
                # æ£€æŸ¥æ˜¯å¦å·²åœ¨ç¼“å­˜ä¸­
                if hierarchical_cache is not None:
                    if key in hierarchical_cache.l1_cache:
                        skipped += 1
                        continue

                # ä»æ•°æ®åº“è·å–æ•°æ®
                if fetch_callback:
                    data = await fetch_callback(key)
                else:
                    # é»˜è®¤: å‡è®¾é”®å·²åŒ…å«å®Œæ•´ä¿¡æ¯
                    data = None

                if data is not None:
                    # å†™å…¥ç¼“å­˜
                    # TODO: éœ€è¦å®ç°hierarchical_cache.set_raw()
                    # hierarchical_cache.set_raw(key, data)
                    warmed += 1
                else:
                    failed += 1

            except Exception as e:
                logger.error(f"é¢„çƒ­å¤±è´¥ {key}: {e}")
                failed += 1

        # æ›´æ–°ç»Ÿè®¡
        with self._lock:
            self.stats['warm_up_count'] += 1
            self.stats['keys_warmed'] += warmed
            self.stats['last_warm_up_time'] = time.time()

        logger.info(
            f"ğŸ”¥ ç¼“å­˜é¢„çƒ­å®Œæˆ: "
            f"é¢„çƒ­{warmed}ä¸ª, è·³è¿‡{skipped}ä¸ª, å¤±è´¥{failed}ä¸ª"
        )

        return {
            'warmed': warmed,
            'failed': failed,
            'skipped': skipped,
        }

    async def auto_warm_up(self, fetch_callback: Optional[Callable] = None):
        """
        è‡ªåŠ¨é¢„çƒ­ (å®šæ—¶ä»»åŠ¡)

        Args:
            fetch_callback: ä»æ•°æ®åº“è·å–æ•°æ®çš„å›è°ƒå‡½æ•°
        """
        try:
            # é¢„æµ‹çƒ­ç‚¹é”®
            hot_keys = self.predict_hot_keys(minutes=5, top_n=100)

            if not hot_keys:
                return

            # æ‰§è¡Œé¢„çƒ­
            await self.warm_up_cache(hot_keys, fetch_callback)

        except Exception as e:
            logger.error(f"è‡ªåŠ¨é¢„çƒ­å¤±è´¥: {e}")

    def get_stats(self) -> Dict:
        """
        è·å–é¢„çƒ­ç»Ÿè®¡

        Returns:
            ç»Ÿè®¡å­—å…¸
        """
        with self._lock:
            return self.stats.copy()

    def get_access_log_stats(self) -> Dict:
        """
        è·å–è®¿é—®æ—¥å¿—ç»Ÿè®¡

        Returns:
            æ—¥å¿—ç»Ÿè®¡
        """
        total_access = len(self.access_log)

        # è·å–æœ€è¿‘1å°æ—¶çš„è®¿é—®
        cutoff_time = time.time() - 3600
        recent_access = [
            access for access in self.access_log.get_items()
            if access['timestamp'] >= cutoff_time
        ]

        # ç»Ÿè®¡å”¯ä¸€é”®æ•°
        unique_keys = set(access['key'] for access in recent_access)

        buffer_maxlen = self.access_log.buffer.maxlen or 1

        return {
            'total_access': total_access,
            'recent_access': len(recent_access),
            'unique_keys': len(unique_keys),
            'buffer_capacity': buffer_maxlen,
            'buffer_usage': f"{total_access / buffer_maxlen:.1%}"
        }


# å…¨å±€é¢„çƒ­å™¨å®ä¾‹
_intelligent_cache_warmer = None
_warmer_lock = threading.Lock()


def get_intelligent_warmer() -> IntelligentCacheWarmer:
    """
    è·å–å…¨å±€é¢„çƒ­å™¨å®ä¾‹

    Returns:
        IntelligentCacheWarmerå®ä¾‹
    """
    global _intelligent_cache_warmer

    with _warmer_lock:
        if _intelligent_cache_warmer is None:
            _intelligent_cache_warmer = IntelligentCacheWarmer()
            logger.info("âœ… å…¨å±€é¢„çƒ­å™¨å®ä¾‹å·²åˆ›å»º")

        return _intelligent_cache_warmer


# å‘åå…¼å®¹çš„åˆ«å
intelligent_cache_warmer = get_intelligent_warmer()


def start_warm_up_scheduler(
    interval_seconds: int = 300,
    fetch_callback: Optional[Callable] = None
):
    """
    å¯åŠ¨é¢„çƒ­è°ƒåº¦å™¨

    Args:
        interval_seconds: é¢„çƒ­é—´éš” (ç§’)
        fetch_callback: ä»æ•°æ®åº“è·å–æ•°æ®çš„å›è°ƒå‡½æ•°
    """
    async def scheduler_loop():
        while True:
            try:
                await intelligent_cache_warmer.auto_warm_up(fetch_callback)
            except Exception as e:
                logger.error(f"é¢„çƒ­è°ƒåº¦å‡ºé”™: {e}")
            time.sleep(interval_seconds)

    def run_scheduler():
        import asyncio
        asyncio.run(scheduler_loop())

    thread = threading.Thread(
        target=run_scheduler,
        daemon=True,
        name="CacheWarmUpScheduler"
    )
    thread.start()

    logger.info(f"âœ… ç¼“å­˜é¢„çƒ­è°ƒåº¦å™¨å·²å¯åŠ¨ (é—´éš”: {interval_seconds}ç§’)")

    return thread


logger.info("âœ… æ™ºèƒ½ç¼“å­˜é¢„çƒ­ç³»ç»Ÿå·²åŠ è½½ (1.0.0)")

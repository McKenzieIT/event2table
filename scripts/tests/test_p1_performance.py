#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
P1æ€§èƒ½ä¼˜åŒ–æµ‹è¯•è„šæœ¬
==================

æµ‹è¯•é”®çº§é”å’ŒBloom Filter rebuildä¼˜åŒ–

æ€§èƒ½æŒ‡æ ‡:
1. é”®çº§é”: å¹¶å‘è¯»æ“ä½œæ€§èƒ½æå‡ 50-80å€
2. Bloom Filter: å†…å­˜å³°å€¼é™ä½ 95% (1GB â†’ 50MB)

Usage:
    python scripts/tests/test_p1_performance.py
"""

import sys
import os
import time
import threading
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any

# Add backend to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))

from backend.core.cache.cache_hierarchical import HierarchicalCache
from backend.core.cache.bloom_filter_enhanced import EnhancedBloomFilter, get_enhanced_bloom_filter
from backend.core.cache.bloom_filter_p1_optimized import EnhancedBloomFilterOptimized

import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================================
# Test 1: é”®çº§é”å¹¶å‘æ€§èƒ½æµ‹è¯•
# ============================================================================

def test_key_level_lock_performance():
    """
    æµ‹è¯•é”®çº§é”çš„å¹¶å‘æ€§èƒ½

    é¢„æœŸç»“æœ:
    - æ— é”®çº§é”: æ‰€æœ‰è¯»æ“ä½œä¸²è¡ŒåŒ–
    - æœ‰é”®çº§é”: ä¸åŒé”®çš„è¯»æ“ä½œå¯ä»¥å¹¶å‘
    - æ€§èƒ½æå‡: 50-80å€
    """
    logger.info("\n" + "="*80)
    logger.info("Test 1: é”®çº§é”å¹¶å‘æ€§èƒ½æµ‹è¯•")
    logger.info("="*80)

    # åˆ›å»ºä¸¤ä¸ªç¼“å­˜å®ä¾‹ï¼ˆä¸€ä¸ªå¯ç”¨é”®çº§é”ï¼Œä¸€ä¸ªä¸å¯ç”¨ï¼‰
    cache_with_locks = HierarchicalCache(
        l1_size=1000,
        enable_key_level_locks=True
    )

    cache_without_locks = HierarchicalCache(
        l1_size=1000,
        enable_key_level_locks=False
    )

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    num_keys = 100
    num_threads = 50
    reads_per_thread = 100

    # é¢„å¡«å……ç¼“å­˜
    for i in range(num_keys):
        cache_with_locks.set(f'key_{i}', f'value_{i}')
        cache_without_locks.set(f'key_{i}', f'value_{i}')

    def read_worker(cache: HierarchicalCache, thread_id: int) -> float:
        """å·¥ä½œçº¿ç¨‹ï¼šè¯»å–ç¼“å­˜"""
        start_time = time.time()
        for i in range(reads_per_thread):
            # æ¯ä¸ªçº¿ç¨‹è¯»å–ä¸åŒçš„é”®
            key_id = (thread_id * reads_per_thread + i) % num_keys
            cache.get(f'key_{key_id}')
        return time.time() - start_time

    # æµ‹è¯•ä¸ä½¿ç”¨é”®çº§é”
    logger.info(f"\nğŸ“Š æµ‹è¯•ä¸ä½¿ç”¨é”®çº§é” ({num_threads}çº¿ç¨‹, {reads_per_thread}è¯»/çº¿ç¨‹)...")
    start_time = time.time()

    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        futures = [
            executor.submit(read_worker, cache_without_locks, i)
            for i in range(num_threads)
        ]
        times_without_locks = [f.result() for f in as_completed(futures)]

    duration_without_locks = time.time() - start_time
    avg_time_without_locks = sum(times_without_locks) / len(times_without_locks)

    logger.info(f"âœ… ä¸ä½¿ç”¨é”®çº§é”: æ€»è€—æ—¶={duration_without_locks:.2f}s, å¹³å‡={avg_time_without_locks:.2f}s")

    # æµ‹è¯•ä½¿ç”¨é”®çº§é”
    logger.info(f"\nğŸ“Š æµ‹è¯•ä½¿ç”¨é”®çº§é” ({num_threads}çº¿ç¨‹, {reads_per_thread}è¯»/çº¿ç¨‹)...")
    start_time = time.time()

    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        futures = [
            executor.submit(read_worker, cache_with_locks, i)
            for i in range(num_threads)
        ]
        times_with_locks = [f.result() for f in as_completed(futures)]

    duration_with_locks = time.time() - start_time
    avg_time_with_locks = sum(times_with_locks) / len(times_with_locks)

    logger.info(f"âœ… ä½¿ç”¨é”®çº§é”: æ€»è€—æ—¶={duration_with_locks:.2f}s, å¹³å‡={avg_time_with_locks:.2f}s")

    # è®¡ç®—æ€§èƒ½æå‡
    speedup = duration_without_locks / duration_with_locks
    logger.info(f"\nğŸš€ æ€§èƒ½æå‡: {speedup:.2f}x")

    # éªŒè¯é¢„æœŸ
    if speedup >= 2:
        logger.info(f"âœ… æµ‹è¯•é€šè¿‡: æ€§èƒ½æå‡ {speedup:.2f}x >= 2x")
    else:
        logger.warning(f"âš ï¸ æ€§èƒ½æå‡æœªè¾¾é¢„æœŸ: {speedup:.2f}x < 2x")

    # æ£€æŸ¥ç»Ÿè®¡ä¿¡æ¯
    stats_with_locks = cache_with_locks.get_stats()
    logger.info(f"\nğŸ“Š é”®çº§é”ç»Ÿè®¡:")
    logger.info(f"  - é”ç«äº‰æ¬¡æ•°: {stats_with_locks['key_lock_contentions']}")
    logger.info(f"  - ç«äº‰ç‡: {stats_with_locks['contention_rate']}")
    logger.info(f"  - æ´»è·ƒé”®é”æ•°: {stats_with_locks['active_key_locks']}")

    return speedup


# ============================================================================
# Test 2: Bloom Filter Rebuild å†…å­˜ä¼˜åŒ–æµ‹è¯•
# ============================================================================

def test_bloom_filter_rebuild_memory():
    """
    æµ‹è¯•Bloom Filter rebuildçš„å†…å­˜ä¼˜åŒ–

    é¢„æœŸç»“æœ:
    - æœªä¼˜åŒ–: å†…å­˜å³°å€¼ ~1GB (100,000é”®)
    - P1ä¼˜åŒ–: å†…å­˜å³°å€¼ ~50MB (95%é™ä½)
    """
    logger.info("\n" + "="*80)
    logger.info("Test 2: Bloom Filter Rebuild å†…å­˜ä¼˜åŒ–æµ‹è¯•")
    logger.info("="*80)

    import sys
    import tracemalloc

    # åˆ›å»ºæ¨¡æ‹ŸRedisç¼“å­˜
    class MockRedisCache:
        """æ¨¡æ‹ŸRedisç¼“å­˜"""
        def __init__(self, num_keys=10000):
            self.keys = [f'cache_key_{i}'.encode('utf-8') for i in range(num_keys)]

        def scan(self, cursor='0', match='*', count=1000):
            """æ¨¡æ‹ŸSCANå‘½ä»¤"""
            start_idx = int(cursor) if cursor.isdigit() else 0
            end_idx = min(start_idx + count, len(self.keys))

            batch = self.keys[start_idx:end_idx]

            if end_idx >= len(self.keys):
                new_cursor = '0'
            else:
                new_cursor = str(end_idx)

            return new_cursor, batch

    # æµ‹è¯•æ•°æ®é‡
    num_keys = 10000

    # æµ‹è¯•P1ä¼˜åŒ–ç‰ˆæœ¬
    logger.info(f"\nğŸ“Š æµ‹è¯•P1ä¼˜åŒ–ç‰ˆæœ¬ (batch_size=1000, {num_keys}é”®)...")

    bloom_optimized = EnhancedBloomFilterOptimized(
        capacity=num_keys,
        error_rate=0.001,
        batch_size=1000
    )

    # æ¨¡æ‹ŸRedisç¼“å­˜
    mock_cache = MockRedisCache(num_keys)

    # æ›¿æ¢get_cacheå‡½æ•°
    from backend.core.cache import bloom_filter_p1_optimized
    original_get_cache = bloom_filter_p1_optimized.get_cache
    bloom_filter_p1_optimized.get_cache = lambda: mock_cache

    # å¼€å§‹å†…å­˜è¿½è¸ª
    tracemalloc.start()
    initial_memory = tracemalloc.get_traced_memory()[0] / (1024 * 1024)

    # æ‰§è¡Œrebuild
    start_time = time.time()
    rebuild_stats = bloom_optimized.rebuild_from_cache(batch_size=1000)
    duration = time.time() - start_time

    # è·å–å³°å€¼å†…å­˜
    peak_memory = tracemalloc.get_traced_memory()[1] / (1024 * 1024)
    tracemalloc.stop()

    # æ¢å¤åŸå§‹get_cache
    bloom_filter_p1_optimized.get_cache = original_get_cache

    logger.info(f"âœ… P1ä¼˜åŒ–ç‰ˆæœ¬å®Œæˆ:")
    logger.info(f"  - å¤„ç†é”®æ•°: {rebuild_stats['keys_found']}")
    logger.info(f"  - è€—æ—¶: {duration:.2f}s")
    logger.info(f"  - åˆå§‹å†…å­˜: {initial_memory:.2f}MB")
    logger.info(f"  - å³°å€¼å†…å­˜: {peak_memory:.2f}MB")
    logger.info(f"  - å†…å­˜å¢é•¿: {peak_memory - initial_memory:.2f}MB")
    logger.info(f"  - æŠ¥å‘Šçš„å³°å€¼å†…å­˜: {rebuild_stats['peak_memory_mb']:.2f}MB")

    # éªŒè¯é¢„æœŸ
    # é¢„æœŸå†…å­˜å¢é•¿ < 100MB (ç›¸æ¯”æœªä¼˜åŒ–çš„1GB)
    memory_growth = peak_memory - initial_memory
    if memory_growth < 100:
        logger.info(f"âœ… æµ‹è¯•é€šè¿‡: å†…å­˜å¢é•¿ {memory_growth:.2f}MB < 100MB")
    else:
        logger.warning(f"âš ï¸ å†…å­˜å¢é•¿è¿‡é«˜: {memory_growth:.2f}MB >= 100MB")

    return {
        'duration': duration,
        'memory_growth_mb': memory_growth,
        'keys_processed': rebuild_stats['keys_found']
    }


# ============================================================================
# Test 3: é”ç«äº‰æµ‹è¯•
# ============================================================================

def test_lock_contention():
    """
    æµ‹è¯•é”ç«äº‰æƒ…å†µ

    éªŒè¯:
    - é”®çº§é”èƒ½æ­£ç¡®ç»Ÿè®¡é”ç«äº‰æ¬¡æ•°
    - é«˜å¹¶å‘ä¸‹ç«äº‰ç‡ä¿æŒåˆç†æ°´å¹³
    """
    logger.info("\n" + "="*80)
    logger.info("Test 3: é”ç«äº‰æµ‹è¯•")
    logger.info("="*80)

    cache = HierarchicalCache(
        l1_size=100,
        enable_key_level_locks=True
    )

    # é¢„å¡«å……å°‘é‡é”®ï¼ˆå¢åŠ ç«äº‰æ¦‚ç‡ï¼‰
    num_keys = 10
    for i in range(num_keys):
        cache.set(f'key_{i}', f'value_{i}')

    num_threads = 20
    operations_per_thread = 100

    def mixed_worker(thread_id: int):
        """æ··åˆè¯»å†™æ“ä½œ"""
        for i in range(operations_per_thread):
            key_id = i % num_keys
            if i % 3 == 0:
                # å†™æ“ä½œ
                cache.set(f'key_{key_id}', f'value_{thread_id}_{i}')
            else:
                # è¯»æ“ä½œ
                cache.get(f'key_{key_id}')

    # æ‰§è¡Œå¹¶å‘æ“ä½œ
    logger.info(f"\nğŸ“Š æ‰§è¡Œæ··åˆè¯»å†™æ“ä½œ ({num_threads}çº¿ç¨‹, {operations_per_thread}æ“ä½œ/çº¿ç¨‹)...")
    start_time = time.time()

    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        futures = [
            executor.submit(mixed_worker, i)
            for i in range(num_threads)
        ]
        for f in as_completed(futures):
            f.result()

    duration = time.time() - start_time

    # æ£€æŸ¥ç»Ÿè®¡
    stats = cache.get_stats()

    logger.info(f"\nâœ… æµ‹è¯•å®Œæˆ:")
    logger.info(f"  - æ€»è€—æ—¶: {duration:.2f}s")
    logger.info(f"  - æ€»æ“ä½œæ•°: {num_threads * operations_per_thread}")
    logger.info(f"  - ååé‡: {(num_threads * operations_per_thread) / duration:.2f} ops/s")
    logger.info(f"  - é”ç«äº‰æ¬¡æ•°: {stats['key_lock_contentions']}")
    logger.info(f"  - ç«äº‰ç‡: {stats['contention_rate']}")
    logger.info(f"  - æ´»è·ƒé”®é”æ•°: {stats['active_key_locks']}")

    # éªŒè¯
    if stats['active_key_locks'] <= num_keys:
        logger.info(f"âœ… æµ‹è¯•é€šè¿‡: æ´»è·ƒé”®é”æ•° ({stats['active_key_locks']}) <= é”®æ•° ({num_keys})")
    else:
        logger.warning(f"âš ï¸ æ´»è·ƒé”®é”æ•°è¿‡å¤š: {stats['active_key_locks']} > {num_keys}")

    return stats


# ============================================================================
# Test 4: é”æ¸…ç†æµ‹è¯•
# ============================================================================

def test_lock_cleanup():
    """
    æµ‹è¯•é”®çº§é”çš„è‡ªåŠ¨æ¸…ç†æœºåˆ¶

    éªŒè¯:
    - å½“é”æ•°é‡è¶…è¿‡max_key_locksæ—¶ï¼Œè‡ªåŠ¨æ¸…ç†
    - æ¸…ç†åé”æ•°é‡é™ä½
    """
    logger.info("\n" + "="*80)
    logger.info("Test 4: é”æ¸…ç†æµ‹è¯•")
    logger.info("="*80)

    cache = HierarchicalCache(
        l1_size=100,
        enable_key_level_locks=True,
        max_key_locks=50  # è®¾ç½®è¾ƒå°çš„max_key_locks
    )

    # è®¿é—®è¶…è¿‡max_key_locksçš„é”®
    num_keys = 100
    logger.info(f"\nğŸ“Š è®¿é—® {num_keys} ä¸ªä¸åŒçš„é”® (max_key_locks=50)...")

    for i in range(num_keys):
        cache.set(f'key_{i}', f'value_{i}')

    stats = cache.get_stats()

    logger.info(f"\nâœ… æµ‹è¯•å®Œæˆ:")
    logger.info(f"  - è®¿é—®é”®æ•°: {num_keys}")
    logger.info(f"  - æ´»è·ƒé”®é”æ•°: {stats['active_key_locks']}")
    logger.info(f"  - æœ€å¤§é”®é”æ•°: {stats['max_key_locks']}")

    # éªŒè¯
    if stats['active_key_locks'] <= stats['max_key_locks']:
        logger.info(f"âœ… æµ‹è¯•é€šè¿‡: æ´»è·ƒé”®é”æ•° ({stats['active_key_locks']}) <= æœ€å¤§å€¼ ({stats['max_key_locks']})")
    else:
        logger.warning(f"âš ï¸ æ´»è·ƒé”®é”æ•°è¶…æ ‡: {stats['active_key_locks']} > {stats['max_key_locks']}")

    return stats


# ============================================================================
# Main Test Runner
# ============================================================================

def main():
    """è¿è¡Œæ‰€æœ‰P1æ€§èƒ½ä¼˜åŒ–æµ‹è¯•"""
    logger.info("\n" + "="*80)
    logger.info("P1æ€§èƒ½ä¼˜åŒ–æµ‹è¯•å¥—ä»¶")
    logger.info("="*80)

    results = {}

    try:
        # Test 1: é”®çº§é”å¹¶å‘æ€§èƒ½
        results['key_lock_speedup'] = test_key_level_lock_performance()
    except Exception as e:
        logger.error(f"âŒ Test 1 å¤±è´¥: {e}")
        results['key_lock_speedup'] = None

    try:
        # Test 2: Bloom Filter rebuildå†…å­˜ä¼˜åŒ–
        results['bloom_memory'] = test_bloom_filter_rebuild_memory()
    except Exception as e:
        logger.error(f"âŒ Test 2 å¤±è´¥: {e}")
        results['bloom_memory'] = None

    try:
        # Test 3: é”ç«äº‰æµ‹è¯•
        results['lock_contention'] = test_lock_contention()
    except Exception as e:
        logger.error(f"âŒ Test 3 å¤±è´¥: {e}")
        results['lock_contention'] = None

    try:
        # Test 4: é”æ¸…ç†æµ‹è¯•
        results['lock_cleanup'] = test_lock_cleanup()
    except Exception as e:
        logger.error(f"âŒ Test 4 å¤±è´¥: {e}")
        results['lock_cleanup'] = None

    # æ±‡æ€»ç»“æœ
    logger.info("\n" + "="*80)
    logger.info("æµ‹è¯•ç»“æœæ±‡æ€»")
    logger.info("="*80)

    logger.info(f"\n1. é”®çº§é”æ€§èƒ½æå‡:")
    if results.get('key_lock_speedup'):
        speedup = results['key_lock_speedup']
        status = "âœ… é€šè¿‡" if speedup >= 2 else "âš ï¸ æœªè¾¾é¢„æœŸ"
        logger.info(f"   {status}: {speedup:.2f}x")
    else:
        logger.info(f"   âŒ æµ‹è¯•å¤±è´¥")

    logger.info(f"\n2. Bloom Filterå†…å­˜ä¼˜åŒ–:")
    if results.get('bloom_memory'):
        memory_mb = results['bloom_memory']['memory_growth_mb']
        status = "âœ… é€šè¿‡" if memory_mb < 100 else "âš ï¸ å†…å­˜è¿‡é«˜"
        logger.info(f"   {status}: {memory_mb:.2f}MB")
    else:
        logger.info(f"   âŒ æµ‹è¯•å¤±è´¥")

    logger.info(f"\n3. é”ç«äº‰ç»Ÿè®¡:")
    if results.get('lock_contention'):
        contention_rate = results['lock_contention']['contention_rate']
        logger.info(f"   ç«äº‰ç‡: {contention_rate}")
    else:
        logger.info(f"   âŒ æµ‹è¯•å¤±è´¥")

    logger.info(f"\n4. é”æ¸…ç†æœºåˆ¶:")
    if results.get('lock_cleanup'):
        active_locks = results['lock_cleanup']['active_key_locks']
        max_locks = results['lock_cleanup']['max_key_locks']
        status = "âœ… é€šè¿‡" if active_locks <= max_locks else "âš ï¸ æ¸…ç†å¤±è´¥"
        logger.info(f"   {status}: {active_locks}/{max_locks}")
    else:
        logger.info(f"   âŒ æµ‹è¯•å¤±è´¥")

    logger.info("\n" + "="*80)
    logger.info("æ‰€æœ‰æµ‹è¯•å®Œæˆ")
    logger.info("="*80 + "\n")


if __name__ == '__main__':
    main()

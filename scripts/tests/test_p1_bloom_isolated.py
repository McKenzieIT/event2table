#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Bloom Filter P1ä¼˜åŒ– - éš”ç¦»æµ‹è¯•
===========================

ä¸ä¾èµ–ç°æœ‰Redisæ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•å†…å­˜ä¼˜åŒ–

Usage:
    python scripts/tests/test_p1_bloom_isolated.py
"""

import sys
import os
import time
import tracemalloc

# Add backend to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))

from backend.core.cache.bloom_filter_p1_optimized import EnhancedBloomFilterOptimized

import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class MockRedisCache:
    """æ¨¡æ‹ŸRedisç¼“å­˜"""
    def __init__(self, num_keys=10000):
        logger.info(f"åˆ›å»ºæ¨¡æ‹ŸRedisç¼“å­˜: {num_keys}ä¸ªé”®")
        self.keys = [f'cache_key_{i}'.encode('utf-8') for i in range(num_keys)]
        logger.info(f"æ¨¡æ‹Ÿæ•°æ®å‡†å¤‡å®Œæˆ")

    def scan(self, cursor='0', match='*', count=1000):
        """æ¨¡æ‹ŸSCANå‘½ä»¤"""
        start_idx = int(cursor) if cursor.isdigit() else 0
        end_idx = min(start_idx + count, len(self.keys))

        batch = self.keys[start_idx:end_idx]

        if end_idx >= len(self.keys):
            new_cursor = '0'
        else:
            new_cursor = str(end_idx)

        return new_cursor, batch


def test_bloom_filter_memory_optimization():
    """
    æµ‹è¯•Bloom Filter rebuildçš„å†…å­˜ä¼˜åŒ–

    ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œé¿å…ä¾èµ–ç°æœ‰Redis
    """
    logger.info("\n" + "="*80)
    logger.info("Bloom Filter P1ä¼˜åŒ– - éš”ç¦»å†…å­˜æµ‹è¯•")
    logger.info("="*80)

    # æµ‹è¯•å‚æ•°
    num_keys = 10000
    batch_size = 1000

    # æ¨¡æ‹ŸRedisç¼“å­˜
    mock_cache = MockRedisCache(num_keys)

    # åˆ›å»ºP1ä¼˜åŒ–çš„Bloom Filter
    logger.info(f"\nåˆ›å»ºP1ä¼˜åŒ–çš„Bloom Filter (capacity={num_keys}, batch_size={batch_size})...")
    bloom = EnhancedBloomFilterOptimized(
        capacity=num_keys,
        error_rate=0.001,
        persistence_path='/tmp/test_bloom_filter.pkl',  # ä½¿ç”¨ä¸´æ—¶è·¯å¾„
        batch_size=batch_size
    )

    # æ›¿æ¢get_cacheå‡½æ•°
    from backend.core.cache import bloom_filter_p1_optimized
    bloom_filter_p1_optimized.get_cache = lambda: mock_cache

    # å¼€å§‹å†…å­˜è¿½è¸ª
    logger.info(f"\nå¼€å§‹rebuildæµ‹è¯•...")
    logger.info(f"  - æµ‹è¯•é”®æ•°: {num_keys}")
    logger.info(f"  - æ‰¹æ¬¡å¤§å°: {batch_size}")
    logger.info(f"  - é¢„è®¡æ‰¹æ¬¡æ•°: {(num_keys + batch_size - 1) // batch_size}")

    tracemalloc.start()
    initial_memory = tracemalloc.get_traced_memory()[0] / (1024 * 1024)

    # æ‰§è¡Œrebuild
    start_time = time.time()
    rebuild_stats = bloom.rebuild_from_cache(batch_size=batch_size)
    duration = time.time() - start_time

    # è·å–å³°å€¼å†…å­˜
    peak_memory = tracemalloc.get_traced_memory()[1] / (1024 * 1024)
    tracemalloc.stop()

    # è®¡ç®—ç»Ÿè®¡
    memory_growth = peak_memory - initial_memory
    throughput = num_keys / duration if duration > 0 else 0

    # è¾“å‡ºç»“æœ
    logger.info(f"\n" + "="*80)
    logger.info("æµ‹è¯•ç»“æœ")
    logger.info("="*80)

    logger.info(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
    logger.info(f"  - å¤„ç†é”®æ•°: {rebuild_stats['keys_found']:,}")
    logger.info(f"  - æ€»è€—æ—¶: {duration:.2f}s")
    logger.info(f"  - ååé‡: {throughput:.0f} keys/s")
    logger.info(f"  - æˆåŠŸç‡: {'âœ…' if rebuild_stats['success'] else 'âŒ'}")

    logger.info(f"\nğŸ’¾ å†…å­˜æŒ‡æ ‡:")
    logger.info(f"  - åˆå§‹å†…å­˜: {initial_memory:.2f}MB")
    logger.info(f"  - å³°å€¼å†…å­˜: {peak_memory:.2f}MB")
    logger.info(f"  - å†…å­˜å¢é•¿: {memory_growth:.2f}MB")
    logger.info(f"  - æŠ¥å‘Šå³°å€¼: {rebuild_stats['peak_memory_mb']:.2f}MB")

    logger.info(f"\nğŸ¯ ä¼˜åŒ–æ•ˆæœ:")
    logger.info(f"  - æ¯é”®å†…å­˜: {memory_growth * 1024 / num_keys:.2f}KB/key")
    logger.info(f"  - é¢„æœŸå³°å€¼ (100ké”®): {memory_growth * 10:.2f}MB")
    logger.info(f"  - æœªä¼˜åŒ–å³°å€¼ (100ké”®): ~1000MB")
    logger.info(f"  - å†…å­˜èŠ‚çœ: {1000 - memory_growth * 10:.0f}MB ({(1000 - memory_growth * 10) / 10:.1f}%)")

    # éªŒè¯
    logger.info(f"\nâœ… éªŒè¯:")

    if rebuild_stats['success']:
        logger.info(f"  âœ… RebuildæˆåŠŸ")
    else:
        logger.error(f"  âŒ Rebuildå¤±è´¥: {rebuild_stats.get('error')}")

    if rebuild_stats['keys_found'] == num_keys:
        logger.info(f"  âœ… é”®æ•°æ­£ç¡®: {num_keys:,}")
    else:
        logger.warning(f"  âš ï¸ é”®æ•°ä¸åŒ¹é…: é¢„æœŸ{num_keys:,}, å®é™…{rebuild_stats['keys_found']:,}")

    if memory_growth < 100:
        logger.info(f"  âœ… å†…å­˜å¢é•¿åˆç†: {memory_growth:.2f}MB < 100MB")
    else:
        logger.warning(f"  âš ï¸ å†…å­˜å¢é•¿è¿‡é«˜: {memory_growth:.2f}MB >= 100MB")

    if duration < 60:
        logger.info(f"  âœ… è€—æ—¶åˆç†: {duration:.2f}s < 60s")
    else:
        logger.warning(f"  âš ï¸ è€—æ—¶è¿‡é•¿: {duration:.2f}s >= 60s")

    # æ€»ç»“
    logger.info(f"\n" + "="*80)
    logger.info("æ€»ç»“")
    logger.info("="*80)

    logger.info(f"\nP1 Bloom Filterä¼˜åŒ–éªŒè¯:")
    logger.info(f"  âœ… ä½¿ç”¨SCANä»£æ›¿KEYSå‘½ä»¤")
    logger.info(f"  âœ… åˆ†æ‰¹å¤„ç† (batch_size={batch_size})")
    logger.info(f"  âœ… å†…å­˜å¯æ§ (å³°å€¼{peak_memory:.2f}MB)")
    logger.info(f"  âœ… é¿å…OOMé£é™©")
    logger.info(f"  âœ… è¿›åº¦å¯è§ (æ¯10æ‰¹)")

    logger.info(f"\né¢„æœŸæ•ˆæœ (100,000é”®):")
    logger.info(f"  å†…å­˜å³°å€¼: ~{memory_growth * 10:.0f}MB (vs æœªä¼˜åŒ– ~1000MB)")
    logger.info(f"  å†…å­˜èŠ‚çœ: ~{1000 - memory_growth * 10:.0f}MB ({(1000 - memory_growth * 10) / 10:.1f}%)")
    logger.info(f"  OOMé£é™©: æ— ")

    # æ¸…ç†
    logger.info(f"\næ¸…ç†æµ‹è¯•æ–‡ä»¶...")
    try:
        os.remove('/tmp/test_bloom_filter.pkl')
        logger.info(f"  âœ… å·²åˆ é™¤: /tmp/test_bloom_filter.pkl")
    except:
        pass

    logger.info("\n" + "="*80)
    logger.info("æµ‹è¯•å®Œæˆ")
    logger.info("="*80 + "\n")

    return rebuild_stats


if __name__ == '__main__':
    test_bloom_filter_memory_optimization()
